A) After following the steps outlined in the exercises, my predictions for the first set of data points (drawn from the distributions with means -1 and 1 respectively and a variances of 5) was in the range 47%-73% for 5 different trials. (Accuracy was calculated by taking the number of correct predictions and dividing by the number of total predictions.)
The accuracy of the predictions based on the second set of data points (drawn from distributions with means -4 and 4 respectively and variances of 2) was 100% (over 5 trials). That makes sense because there is no overlap between the points from the 2 distributions. When plotted using the scatterplot we created in the exercise, the blue points cluster to the right of 0 and the orange points cluster to the left, with no overlap between the two. While the normal distribution has infinite support and in different trials we can draw data points from the tails that overlap, that probability is small and the accuracy for this data set is expected to be high. Since the data is clearly separated, the parameter estimates from the data are easier to estimate correctly, enhancing the accuracy of the maximum likelihood estimation.

B) 1. The more data points (all else equal), the higher the accuracy (on average). As the number of data points increases, our estimates for the class mean and class covariance get better (tend to the true means and covariances). This makes the estimate of the likelihood better. (On the other hand, for the second data set where the distributions are clearly separated, increasing the sample size can increase the chance that points come from the tail of the distributions where they would overlap, thus affecting the accuracy negatively - if the overlapping outlier is in the test data, it is very likely to be misclassified, but if the overlapping outlier is in the training data, it can distort the estimates for mean and variance significantly and make the  algorithm's performance worse if we don't mitigate the presence of outliers.)
2. The closer the means are to each other (all else equal), the lower the accuracy (on average, assuming the means are close enough given the standard deviations to generate large amounts of overlapping data).  If the means are very close to each other given their standard deviations, our estimates for mean and SD from the data are likely to be biased, affecting negatively the accuracy of the likelihood estimation. The decision boundary for dataset 2 (larger difference between the means) is much sharper and fewer point fall near the overlap region. 
3. The lower the standard deviation (all else equal), the higher the accuracy (on average). If the data is normally distributed with small standard deviations, it will form more clearly defined clusters and the parameter estimates generated from the data will be closer to the true ones, making the decision boundary clearer and the model more accurate. 
